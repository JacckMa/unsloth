#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¸‹è½½è„šæœ¬ - Qwen2.5-1.5B-Instruct
ä¸‹è½½å®Œæˆåå¯ä¾›è®­ç»ƒè„šæœ¬æœ¬åœ°åŠ è½½ä½¿ç”¨
"""

import os
import sys
import time
from pathlib import Path
from huggingface_hub import snapshot_download
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ä¸‹è½½è·¯å¾„
DOWNLOAD_ROOT = "root/autodl-tmp"
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
LOCAL_MODEL_PATH = os.path.join(DOWNLOAD_ROOT, "models", "Qwen2.5-1.5B-Instruct")

print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {MODEL_NAME}")
print(f"ğŸ“ ä¸‹è½½è·¯å¾„: {LOCAL_MODEL_PATH}")
print(f"{'='*80}")

# åˆ›å»ºä¸‹è½½ç›®å½•
os.makedirs(LOCAL_MODEL_PATH, exist_ok=True)
os.makedirs(os.path.dirname(LOCAL_MODEL_PATH), exist_ok=True)

def download_with_progress():
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„ä¸‹è½½å‡½æ•°"""
    try:
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        start_time = time.time()
        
        # ä½¿ç”¨ huggingface_hub ä¸‹è½½
        snapshot_download(
            repo_id=MODEL_NAME,
            local_dir=LOCAL_MODEL_PATH,
            local_dir_use_symlinks=False,  # ä¸ä½¿ç”¨ç¬¦å·é“¾æ¥ï¼Œç›´æ¥å¤åˆ¶æ–‡ä»¶
            resume_download=True,          # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            cache_dir=None,               # ä¸ä½¿ç”¨ç¼“å­˜ï¼Œç›´æ¥ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•
        )
        
        end_time = time.time()
        download_time = end_time - start_time
        
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        print(f"â±ï¸ ä¸‹è½½è€—æ—¶: {download_time:.2f} ç§’")
        
        # æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶
        model_files = list(Path(LOCAL_MODEL_PATH).rglob("*"))
        print(f"ğŸ“Š å…±ä¸‹è½½ {len(model_files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºä¸»è¦æ–‡ä»¶
        important_files = [
            "config.json",
            "tokenizer.json", 
            "tokenizer_config.json",
            "model.safetensors"
        ]
        
        print(f"\nğŸ“‹ ä¸»è¦æ¨¡å‹æ–‡ä»¶:")
        for file in important_files:
            file_path = Path(LOCAL_MODEL_PATH) / file
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"  âœ… {file} ({size_mb:.1f} MB)")
            else:
                print(f"  â“ {file} (æœªæ‰¾åˆ°)")
        
        # æŸ¥æ‰¾ .safetensors æ–‡ä»¶
        safetensor_files = list(Path(LOCAL_MODEL_PATH).glob("*.safetensors"))
        if safetensor_files:
            print(f"\nğŸ”§ æ¨¡å‹æƒé‡æ–‡ä»¶:")
            for file in safetensor_files:
                size_mb = file.stat().st_size / (1024 * 1024)
                print(f"  âœ… {file.name} ({size_mb:.1f} MB)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def verify_model():
    """éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
    print(f"\nğŸ” éªŒè¯æ¨¡å‹æ–‡ä»¶...")
    
    required_files = [
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json"
    ]
    
    all_good = True
    for file in required_files:
        file_path = Path(LOCAL_MODEL_PATH) / file
        if file_path.exists():
            print(f"  âœ… {file}")
        else:
            print(f"  âŒ {file} ç¼ºå¤±")
            all_good = False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æƒé‡æ–‡ä»¶
    weight_files = (
        list(Path(LOCAL_MODEL_PATH).glob("*.safetensors")) + 
        list(Path(LOCAL_MODEL_PATH).glob("*.bin"))
    )
    
    if weight_files:
        print(f"  âœ… æ¨¡å‹æƒé‡æ–‡ä»¶ ({len(weight_files)} ä¸ª)")
    else:
        print(f"  âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶")
        all_good = False
    
    return all_good

def create_load_script():
    """åˆ›å»ºåŠ è½½è„šæœ¬ç¤ºä¾‹"""
    load_script = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°æ¨¡å‹åŠ è½½ç¤ºä¾‹
"""

from unsloth import FastLanguageModel

# ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="{LOCAL_MODEL_PATH}",  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
    max_seq_length=2048,
    load_in_4bit=False,
    dtype=None,
)

print("âœ… æ¨¡å‹ä»æœ¬åœ°åŠ è½½æˆåŠŸï¼")
print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
print(f"ğŸ·ï¸ æ¨¡å‹åç§°: {{model.config.name_or_path}}")
'''
    
    script_path = os.path.join(DOWNLOAD_ROOT, "load_local_model.py")
    with open(script_path, "w", encoding="utf-8") as f:
        f.write(load_script)
    
    print(f"ğŸ“ å·²åˆ›å»ºåŠ è½½è„šæœ¬: {script_path}")

if __name__ == "__main__":
    print("ğŸ”½ å¼€å§‹ä¸‹è½½æ¨¡å‹...")
    
    # ä¸‹è½½æ¨¡å‹
    success = download_with_progress()
    
    if success:
        # éªŒè¯æ–‡ä»¶
        if verify_model():
            print(f"\nğŸ‰ æ¨¡å‹ä¸‹è½½å’ŒéªŒè¯å®Œæˆï¼")
            
            # åˆ›å»ºåŠ è½½è„šæœ¬
            create_load_script()
            
            print(f"\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
            print(f"1. è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨: model_name='{LOCAL_MODEL_PATH}'")
            print(f"2. æµ‹è¯•åŠ è½½: python {DOWNLOAD_ROOT}/load_local_model.py")
            print(f"3. æ¨¡å‹ä½ç½®: {LOCAL_MODEL_PATH}")
            
        else:
            print(f"\nâš ï¸ æ¨¡å‹æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸‹è½½å®Œæ•´æ€§")
            sys.exit(1)
    else:
        print(f"\nâŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
        sys.exit(1)
    
    print(f"\nâœ¨ ä¸‹è½½è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼") 