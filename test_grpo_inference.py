#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GRPOè®­ç»ƒåçš„æ¨¡å‹æ¨ç†æµ‹è¯•
åŠ è½½å¾®è°ƒåçš„ Qwen2.5-1.5B-Instruct æ¨¡å‹è¿›è¡Œæ•°å­¦æ¨ç†
"""

import os
import re
import json
import torch
from pathlib import Path
from datasets import load_dataset
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®è·¯å¾„ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
SAVE_ROOT = "/root/autodl-tmp"
MODEL_SAVE_PATH = os.path.join(SAVE_ROOT, "qwen25_grpo_model")
DATA_CACHE_PATH = os.path.join(SAVE_ROOT, "gsm8k_cache")

# æ ¼å¼åŒ–æ ‡ç­¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
reasoning_start = "<start_working_out>"
reasoning_end = "<end_working_out>"
solution_start = "<SOLUTION>"
solution_end = "</SOLUTION>"

print(f"ğŸš€ GRPOå¾®è°ƒæ¨¡å‹æ¨ç†æµ‹è¯•")
print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_SAVE_PATH}")
print("="*60)

# ============================================================================
# 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
# ============================================================================

def check_model_exists():
    """æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(MODEL_SAVE_PATH) and os.path.exists(os.path.join(MODEL_SAVE_PATH, "adapter_config.json")):
        print(f"âœ… å‘ç°è®­ç»ƒå¥½çš„æ¨¡å‹: {MODEL_SAVE_PATH}")
        return True
    else:
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {MODEL_SAVE_PATH}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬: python grpo_qwen25_gsm8k.py")
        return False

if not check_model_exists():
    exit(1)

# ============================================================================
# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
# ============================================================================

print("\nğŸ”§ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")

from unsloth import FastLanguageModel

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_SAVE_PATH,  # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    max_seq_length=2048,
    load_in_4bit=True,  # æ¨ç†æ—¶å¯ä»¥ä½¿ç”¨4bitèŠ‚çœæ˜¾å­˜
    fast_inference=True,
)

# è®¾ç½®chat templateï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
system_prompt = f"""You are given a problem.
Think about the problem and provide your working out.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between {solution_start}{solution_end}"""

chat_template = (
    "{% if messages[0]['role'] == 'system' %}"
    "{{ messages[0]['content'] + eos_token }}"
    "{% set loop_messages = messages[1:] %}"
    "{% else %}"
    "{{ '{system_prompt}' + eos_token }}"
    "{% set loop_messages = messages %}"
    "{% endif %}"
    "{% for message in loop_messages %}"
    "{% if message['role'] == 'user' %}"
    "{{ message['content'] }}"
    "{% elif message['role'] == 'assistant' %}"
    "{{ message['content'] + eos_token }}"
    "{% endif %}"
    "{% endfor %}"
    "{% if add_generation_prompt %}{{ '{reasoning_start}' }}"
    "{% endif %}"
)

chat_template = chat_template.replace(
    "'{system_prompt}'", f"'{system_prompt}'"
).replace("'{reasoning_start}'", f"'{reasoning_start}'")
tokenizer.chat_template = chat_template

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ============================================================================
# 3. å‡†å¤‡æµ‹è¯•æ•°æ®
# ============================================================================

print("\nğŸ“š å‡†å¤‡æµ‹è¯•æ•°æ®...")

# åŠ è½½GSM8Kæµ‹è¯•é›†
try:
    test_dataset = load_dataset("gsm8k", "main", split="test", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(test_dataset)} æ¡æµ‹è¯•æ•°æ®")
except Exception as e:
    print(f"âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")
    test_dataset = load_dataset("openai/gsm8k", "main", split="test", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(test_dataset)} æ¡æµ‹è¯•æ•°æ®")

# ============================================================================
# 4. æ¨ç†å‡½æ•°å®šä¹‰
# ============================================================================

def extract_answer_from_gsm8k(text):
    """ä»GSM8Kæ ¼å¼ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

def extract_solution_from_response(response):
    """ä»æ¨¡å‹å›ç­”ä¸­æå–è§£å†³æ–¹æ¡ˆ"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è§£å†³æ–¹æ¡ˆ
    match = re.search(rf"{solution_start}(.+?){solution_end}", response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None

def extract_reasoning_from_response(response):
    """ä»æ¨¡å‹å›ç­”ä¸­æå–æ¨ç†è¿‡ç¨‹"""
    # åŒ¹é…æ¨ç†è¿‡ç¨‹
    match = re.search(rf"{reasoning_start}(.*?){reasoning_end}", response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None

def run_inference(question, max_new_tokens=512):
    """è¿è¡Œå•ä¸ªé—®é¢˜çš„æ¨ç†"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ]
    
    # åº”ç”¨chat template
    prompt = tokenizer.apply_chat_template(
        messages, 
        add_generation_prompt=True, 
        tokenize=False
    )
    
    # ç”Ÿæˆå›ç­”
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç å›ç­”
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response

# ============================================================================
# 5. æ‰¹é‡æµ‹è¯•
# ============================================================================

def evaluate_model(num_samples=20):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nğŸ§ª å¼€å§‹è¯„ä¼°æ¨¡å‹ (ä½¿ç”¨å‰{num_samples}ä¸ªæ ·æœ¬)...")
    
    correct_count = 0
    format_correct_count = 0
    total_count = min(num_samples, len(test_dataset))
    
    results = []
    
    for i in range(total_count):
        example = test_dataset[i]
        question = example['question']
        true_answer = extract_answer_from_gsm8k(example['answer'])
        
        print(f"\nğŸ“ æµ‹è¯•æ ·æœ¬ {i+1}/{total_count}")
        print(f"é—®é¢˜: {question}")
        print(f"æ­£ç¡®ç­”æ¡ˆ: {true_answer}")
        
        try:
            # è¿è¡Œæ¨ç†
            response = run_inference(question)
            
            # æå–æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆ
            reasoning = extract_reasoning_from_response(response)
            predicted_answer = extract_solution_from_response(response)
            
            print(f"æ¨¡å‹å›ç­”: {response}")
            print(f"æå–çš„æ¨ç†: {reasoning}")
            print(f"æå–çš„ç­”æ¡ˆ: {predicted_answer}")
            
            # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
            format_ok = (reasoning is not None and predicted_answer is not None)
            if format_ok:
                format_correct_count += 1
                print("âœ… æ ¼å¼æ­£ç¡®")
            else:
                print("âŒ æ ¼å¼é”™è¯¯")
            
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            answer_correct = False
            if predicted_answer and true_answer:
                # å°è¯•æ•°å€¼æ¯”è¾ƒ
                try:
                    pred_num = float(predicted_answer.replace(',', '').strip())
                    true_num = float(true_answer.replace(',', '').strip())
                    answer_correct = abs(pred_num - true_num) < 0.001
                except:
                    # å­—ç¬¦ä¸²æ¯”è¾ƒ
                    answer_correct = predicted_answer.strip() == true_answer.strip()
            
            if answer_correct:
                correct_count += 1
                print("âœ… ç­”æ¡ˆæ­£ç¡®")
            else:
                print("âŒ ç­”æ¡ˆé”™è¯¯")
            
            # ä¿å­˜ç»“æœ
            results.append({
                "question": question,
                "true_answer": true_answer,
                "response": response,
                "reasoning": reasoning,
                "predicted_answer": predicted_answer,
                "format_correct": format_ok,
                "answer_correct": answer_correct
            })
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            results.append({
                "question": question,
                "true_answer": true_answer,
                "error": str(e)
            })
        
        print("-" * 50)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    format_accuracy = format_correct_count / total_count * 100
    answer_accuracy = correct_count / total_count * 100
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"{'='*50}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {total_count}")
    print(f"æ ¼å¼æ­£ç¡®ç‡: {format_correct_count}/{total_count} ({format_accuracy:.1f}%)")
    print(f"ç­”æ¡ˆæ­£ç¡®ç‡: {correct_count}/{total_count} ({answer_accuracy:.1f}%)")
    print(f"{'='*50}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(SAVE_ROOT, "inference_results.json")
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "summary": {
                "total_samples": total_count,
                "format_correct": format_correct_count,
                "answer_correct": correct_count,
                "format_accuracy": format_accuracy,
                "answer_accuracy": answer_accuracy
            },
            "details": results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return format_accuracy, answer_accuracy

# ============================================================================
# 6. äº¤äº’å¼æµ‹è¯•
# ============================================================================

def interactive_test():
    """äº¤äº’å¼æµ‹è¯•æ¨¡å¼"""
    print(f"\nğŸ¯ äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("è¾“å…¥æ•°å­¦é—®é¢˜ï¼Œæ¨¡å‹å°†ç”Ÿæˆæ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆ")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("-" * 50)
    
    while True:
        question = input("\nğŸ“ è¯·è¾“å…¥æ•°å­¦é—®é¢˜: ").strip()
        
        if question.lower() in ['quit', 'exit', 'é€€å‡º']:
            break
        
        if not question:
            continue
        
        try:
            print("\nğŸ¤– æ¨¡å‹æ€è€ƒä¸­...")
            response = run_inference(question)
            
            reasoning = extract_reasoning_from_response(response)
            answer = extract_solution_from_response(response)
            
            print(f"\nğŸ“‹ å®Œæ•´å›ç­”:")
            print(response)
            
            print(f"\nğŸ§  æ¨ç†è¿‡ç¨‹:")
            print(reasoning if reasoning else "æœªæå–åˆ°æ¨ç†è¿‡ç¨‹")
            
            print(f"\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:")
            print(answer if answer else "æœªæå–åˆ°ç­”æ¡ˆ")
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        
        print("-" * 50)

# ============================================================================
# 7. ä¸»ç¨‹åº
# ============================================================================

if __name__ == "__main__":
    print("\nğŸ® è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. æ‰¹é‡è¯„ä¼° (é»˜è®¤20ä¸ªæ ·æœ¬)")
    print("2. äº¤äº’å¼æµ‹è¯•")
    print("3. å•ä¸ªæ ·æœ¬æµ‹è¯•")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice == "2":
        interactive_test()
    elif choice == "3":
        # å•ä¸ªæ ·æœ¬æµ‹è¯•
        sample = test_dataset[0]
        question = sample['question']
        true_answer = extract_answer_from_gsm8k(sample['answer'])
        
        print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {question}")
        print(f"ğŸ“Š æ­£ç¡®ç­”æ¡ˆ: {true_answer}")
        print("\nğŸ¤– æ¨¡å‹æ¨ç†ä¸­...")
        
        response = run_inference(question)
        reasoning = extract_reasoning_from_response(response)
        predicted_answer = extract_solution_from_response(response)
        
        print(f"\nğŸ“‹ å®Œæ•´å›ç­”:\n{response}")
        print(f"\nğŸ§  æå–çš„æ¨ç†:\n{reasoning}")
        print(f"\nğŸ’¡ æå–çš„ç­”æ¡ˆ: {predicted_answer}")
        print(f"âœ… æ­£ç¡®ç­”æ¡ˆ: {true_answer}")
        
    else:
        # é»˜è®¤æ‰¹é‡è¯„ä¼°
        num_samples = 20
        try:
            custom_num = input(f"è¯·è¾“å…¥æµ‹è¯•æ ·æœ¬æ•° (é»˜è®¤{num_samples}): ").strip()
            if custom_num:
                num_samples = int(custom_num)
        except:
            pass
        
        evaluate_model(num_samples)
    
    print("\nâœ¨ æ¨ç†æµ‹è¯•å®Œæˆï¼") 