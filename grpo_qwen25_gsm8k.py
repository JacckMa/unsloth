#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GRPO LoRAå¾®è°ƒ Qwen2.5-1.5B-Instruct æ¨¡å‹
æ•°æ®é›†: GSM8K
ä¿å­˜è·¯å¾„: /root/autodl-tmp
"""

import os
import re
import json
import torch
import gc
from pathlib import Path
from datasets import load_dataset
from transformers import TrainingArguments
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ä¿å­˜æ ¹ç›®å½•
SAVE_ROOT = "/root/autodl-tmp"
os.makedirs(SAVE_ROOT, exist_ok=True)

# è®¾ç½®å„ç§ä¿å­˜è·¯å¾„
MODEL_SAVE_PATH = os.path.join(SAVE_ROOT, "qwen25_grpo_model")
CHECKPOINT_PATH = os.path.join(SAVE_ROOT, "qwen25_grpo_checkpoint") 
DATA_CACHE_PATH = os.path.join(SAVE_ROOT, "gsm8k_cache")
LOG_PATH = os.path.join(SAVE_ROOT, "training_logs")

# åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
for path in [MODEL_SAVE_PATH, CHECKPOINT_PATH, DATA_CACHE_PATH, LOG_PATH]:
    os.makedirs(path, exist_ok=True)

print(f"ğŸš€ å¼€å§‹GRPOå¾®è°ƒ Qwen2.5-1.5B-Instruct")
print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")
print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„: {CHECKPOINT_PATH}")
print(f"ğŸ“Š æ—¥å¿—ä¿å­˜è·¯å¾„: {LOG_PATH}")
print(f"ğŸ—‚ï¸ æ•°æ®ç¼“å­˜è·¯å¾„: {DATA_CACHE_PATH}")

# ============================================================================
# 1. æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½
# ============================================================================

from unsloth import FastLanguageModel

max_seq_length = 2048
lora_rank = 32  # é€‚ä¸­çš„rankï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ

print(f"\n{'='*60}")
print("ğŸ”§ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
print(f"{'='*60}")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    max_seq_length=max_seq_length,
    load_in_4bit=True,  # 4bité‡åŒ–èŠ‚çœæ˜¾å­˜
    fast_inference=False,  # GRPOè®­ç»ƒæ—¶å…³é—­
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.8,
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ============================================================================
# 2. LoRAé…ç½®
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ¯ é…ç½®LoRA...")
print(f"{'='*60}")

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
    max_seq_length=max_seq_length,
    use_rslora=False,
)

print("âœ… LoRAé…ç½®å®Œæˆï¼")

# ============================================================================
# 3. æ•°æ®é›†å‡†å¤‡
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ“š å‡†å¤‡GSM8Kæ•°æ®é›†...")
print(f"{'='*60}")

# å®šä¹‰æ ¼å¼åŒ–æ ‡ç­¾
reasoning_start = "<reasoning>"
reasoning_end = "</reasoning>"
solution_start = "<answer>"
solution_end = "</answer>"

def extract_answer_from_gsm8k(text):
    """ä»GSM8Kæ ¼å¼ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

def format_gsm8k_for_grpo(example):
    """å°†GSM8Kæ ¼å¼åŒ–ä¸ºGRPOè®­ç»ƒæ ¼å¼"""
    system_prompt = (
        f"You are a helpful math tutor. When solving problems, think step by step. "
        f"Place your reasoning between {reasoning_start} and {reasoning_end}. "
        f"Then provide your final numerical answer between {solution_start} and {solution_end}."
    )
    
    return {
        "prompt": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Problem: {example['question']}\n\nSolve this step by step."},
        ],
        "answer": extract_answer_from_gsm8k(example["answer"]),
        "full_solution": example["answer"]  # ä¿ç•™å®Œæ•´è§£ç­”ç”¨äºå‚è€ƒ
    }

# åŠ è½½GSM8Kæ•°æ®é›†
print("ğŸ“¥ ä¸‹è½½GSM8Kæ•°æ®é›†...")
try:
    dataset = load_dataset("gsm8k", "main", split="train", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} æ¡è®­ç»ƒæ•°æ®")
except Exception as e:
    print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
    dataset = load_dataset("openai/gsm8k", "main", split="train", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} æ¡è®­ç»ƒæ•°æ®")

# æ ¼å¼åŒ–æ•°æ®é›†
print("ğŸ”„ æ ¼å¼åŒ–æ•°æ®é›†...")
formatted_dataset = dataset.map(format_gsm8k_for_grpo, num_proc=4)

# åªä½¿ç”¨å‰1000æ¡æ•°æ®è¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
train_size = min(1000, len(formatted_dataset))
gsm8k_train = formatted_dataset.select(range(train_size))

print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼ä½¿ç”¨ {len(gsm8k_train)} æ¡æ•°æ®è¿›è¡Œè®­ç»ƒ")

# ============================================================================
# 4. å¥–åŠ±å‡½æ•°å®šä¹‰
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ¯ å®šä¹‰å¥–åŠ±å‡½æ•°...")
print(f"{'='*60}")

def format_checker_exact(prompts, completions, ground_truth_answers, **kwargs):
    """æ£€æŸ¥æ˜¯å¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡º"""
    scores = []
    for completion in completions:
        completion_text = completion.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€éœ€çš„æ ‡ç­¾
        has_reasoning = reasoning_start in completion_text and reasoning_end in completion_text
        has_answer = solution_start in completion_text and solution_end in completion_text
        
        if has_reasoning and has_answer:
            scores.append(1.0)
        else:
            scores.append(0.0)
    
    return scores

def format_checker_flexible(prompts, completions, ground_truth_answers, **kwargs):
    """çµæ´»çš„æ ¼å¼æ£€æŸ¥"""
    scores = []
    for completion in completions:
        completion_text = completion.strip().lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨ç†ç›¸å…³å…³é”®è¯
        reasoning_keywords = ["reasoning", "think", "step", "because", "therefore", "since"]
        answer_keywords = ["answer", "result", "solution", "final"]
        
        has_reasoning_content = any(keyword in completion_text for keyword in reasoning_keywords)
        has_answer_content = any(keyword in completion_text for keyword in answer_keywords)
        
        if has_reasoning_content and has_answer_content:
            scores.append(0.8)
        elif has_reasoning_content or has_answer_content:
            scores.append(0.4)
        else:
            scores.append(0.1)
    
    return scores

def answer_correctness_checker(prompts, completions, ground_truth_answers, **kwargs):
    """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§"""
    scores = []
    
    for completion, gt_answer in zip(completions, ground_truth_answers):
        if gt_answer is None:
            scores.append(0.0)
            continue
            
        completion_text = completion.strip()
        
        # å°è¯•ä»completionä¸­æå–æ•°å­—ç­”æ¡ˆ
        # ä¼˜å…ˆä»<answer>æ ‡ç­¾ä¸­æå–
        answer_match = re.search(rf'{re.escape(solution_start)}(.*?){re.escape(solution_end)}', 
                                completion_text, re.DOTALL)
        if answer_match:
            predicted_text = answer_match.group(1).strip()
        else:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»æ•´ä¸ªæ–‡æœ¬ä¸­æå–æœ€åçš„æ•°å­—
            predicted_text = completion_text
        
        # æå–æ•°å­—
        predicted_numbers = re.findall(r'-?\d+(?:\.\d+)?', predicted_text)
        gt_numbers = re.findall(r'-?\d+(?:\.\d+)?', str(gt_answer))
        
        if predicted_numbers and gt_numbers:
            try:
                predicted_num = float(predicted_numbers[-1])  # å–æœ€åä¸€ä¸ªæ•°å­—
                gt_num = float(gt_numbers[-1])
                
                if abs(predicted_num - gt_num) < 1e-6:  # æ•°å€¼ç›¸ç­‰
                    scores.append(1.0)
                else:
                    scores.append(0.0)
            except ValueError:
                scores.append(0.0)
        else:
            scores.append(0.0)
    
    return scores

def reasoning_quality_checker(prompts, completions, ground_truth_answers, **kwargs):
    """æ£€æŸ¥æ¨ç†è´¨é‡"""
    scores = []
    
    for completion in completions:
        completion_text = completion.strip()
        
        # åŸºç¡€åˆ†æ•°
        score = 0.3
        
        # æ£€æŸ¥æ¨ç†é•¿åº¦ï¼ˆæ›´é•¿çš„æ¨ç†é€šå¸¸æ›´è¯¦ç»†ï¼‰
        if len(completion_text) > 100:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦ç›¸å…³è¯æ±‡
        math_keywords = ["multiply", "divide", "add", "subtract", "calculate", 
                        "equation", "solve", "total", "sum", "difference"]
        math_count = sum(1 for keyword in math_keywords if keyword in completion_text.lower())
        score += min(math_count * 0.1, 0.3)
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
        logic_keywords = ["therefore", "so", "thus", "hence", "because", "since", 
                         "first", "then", "next", "finally"]
        logic_count = sum(1 for keyword in logic_keywords if keyword in completion_text.lower())
        score += min(logic_count * 0.05, 0.2)
        
        scores.append(min(score, 1.0))
    
    return scores

print("âœ… å¥–åŠ±å‡½æ•°å®šä¹‰å®Œæˆï¼")

# ============================================================================
# 5. GRPOè®­ç»ƒé…ç½®
# ============================================================================

print(f"\n{'='*60}")
print("âš™ï¸ é…ç½®GRPOè®­ç»ƒå‚æ•°...")
print(f"{'='*60}")

# è®¡ç®—æœ€å¤§æç¤ºé•¿åº¦
sample_prompts = [tokenizer.apply_chat_template(gsm8k_train[i]["prompt"], 
                                               add_generation_prompt=True, 
                                               tokenize=True) 
                 for i in range(min(100, len(gsm8k_train)))]
prompt_lengths = [len(prompt) for prompt in sample_prompts]
max_prompt_length = max(prompt_lengths) + 50  # æ·»åŠ ä¸€äº›ç¼“å†²
max_completion_length = max_seq_length - max_prompt_length

print(f"ğŸ“ æœ€å¤§æç¤ºé•¿åº¦: {max_prompt_length}")
print(f"ğŸ“ æœ€å¤§å®Œæˆé•¿åº¦: {max_completion_length}")

from trl import GRPOConfig, GRPOTrainer

training_args = GRPOConfig(
    # åŸºç¡€è®­ç»ƒå‚æ•°
    learning_rate=3e-6,  # è¾ƒå°çš„å­¦ä¹ ç‡ç¡®ä¿ç¨³å®šè®­ç»ƒ
    weight_decay=0.01,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    optim="adamw_8bit",  # 8bitä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
    
    # æ‰¹æ¬¡å’Œæ¢¯åº¦å‚æ•°
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    max_grad_norm=1.0,
    
    # GRPOç‰¹å®šå‚æ•°
    num_generations=4,  # æ¯æ¬¡ç”Ÿæˆ4ä¸ªå€™é€‰ç­”æ¡ˆ
    max_prompt_length=max_prompt_length,
    max_completion_length=max_completion_length,
    
    # è®­ç»ƒæ­¥æ•°å’Œä¿å­˜
    max_steps=500,  # é€‚ä¸­çš„è®­ç»ƒæ­¥æ•°
    save_steps=100,
    logging_steps=10,
    eval_steps=100,
    
    # è¾“å‡ºå’Œæ—¥å¿—
    output_dir=CHECKPOINT_PATH,
    logging_dir=LOG_PATH,
    report_to="tensorboard",  # ä½¿ç”¨tensorboardè®°å½•æ—¥å¿—
    
    # å…¶ä»–å‚æ•°
    dataloader_num_workers=4,
    remove_unused_columns=False,
    seed=42,
)

print("âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼")

# ============================================================================
# 6. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
# ============================================================================

print(f"\n{'='*60}")
print("ğŸš‚ åˆ›å»ºGRPOè®­ç»ƒå™¨...")
print(f"{'='*60}")

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[
        format_checker_exact,      # ä¸¥æ ¼æ ¼å¼æ£€æŸ¥ (æƒé‡é«˜)
        format_checker_flexible,   # çµæ´»æ ¼å¼æ£€æŸ¥
        answer_correctness_checker, # ç­”æ¡ˆæ­£ç¡®æ€§æ£€æŸ¥ (æƒé‡æœ€é«˜)
        reasoning_quality_checker,  # æ¨ç†è´¨é‡æ£€æŸ¥
    ],
    args=training_args,
    train_dataset=gsm8k_train,
)

print("âœ… è®­ç»ƒå™¨åˆ›å»ºå®Œæˆï¼")

# ============================================================================
# 7. å¼€å§‹è®­ç»ƒ
# ============================================================================

print(f"\n{'='*60}")
print(f"ğŸš€ å¼€å§‹GRPOè®­ç»ƒï¼")
print(f"ğŸ“Š è®­ç»ƒæ•°æ®é‡: {len(gsm8k_train)}")
print(f"ğŸ”„ æœ€å¤§è®­ç»ƒæ­¥æ•°: {training_args.max_steps}")
print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”: {training_args.save_steps} æ­¥")
print(f"{'='*60}")

try:
    trainer.train()
    print("âœ… è®­ç»ƒå®Œæˆï¼")
except Exception as e:
    print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    print("ğŸ’¾ å°è¯•ä¿å­˜å½“å‰çŠ¶æ€...")

# ============================================================================
# 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
print(f"{'='*60}")

try:
    # ä¿å­˜LoRAé€‚é…å™¨
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)
    print(f"âœ… LoRAæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—æ€»ç»“
    log_summary = {
        "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
        "dataset": "GSM8K",
        "training_method": "GRPO + LoRA",
        "lora_rank": lora_rank,
        "max_seq_length": max_seq_length,
        "training_samples": len(gsm8k_train),
        "max_steps": training_args.max_steps,
        "learning_rate": training_args.learning_rate,
        "save_paths": {
            "model": MODEL_SAVE_PATH,
            "checkpoint": CHECKPOINT_PATH,
            "logs": LOG_PATH,
            "data_cache": DATA_CACHE_PATH
        }
    }
    
    with open(os.path.join(SAVE_ROOT, "training_summary.json"), "w", encoding="utf-8") as f:
        json.dump(log_summary, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ°: {os.path.join(SAVE_ROOT, 'training_summary.json')}")
    
except Exception as e:
    print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

# ============================================================================
# 9. æ¸…ç†å†…å­˜
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ§¹ æ¸…ç†å†…å­˜...")
print(f"{'='*60}")

del trainer
del model
del tokenizer
torch.cuda.empty_cache()
gc.collect()

print("âœ… å†…å­˜æ¸…ç†å®Œæˆï¼")

# ============================================================================
# 10. è®­ç»ƒå®Œæˆæ€»ç»“
# ============================================================================

print(f"\n{'='*80}")
print("ğŸ‰ GRPOå¾®è°ƒå®Œæˆï¼")
print(f"{'='*80}")
print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶éƒ½ä¿å­˜åœ¨: {SAVE_ROOT}")
print(f"ğŸ·ï¸ æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")
print(f"ğŸ’¾ æ£€æŸ¥ç‚¹è·¯å¾„: {CHECKPOINT_PATH}")
print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—è·¯å¾„: {LOG_PATH}")
print(f"ğŸ—‚ï¸ æ•°æ®ç¼“å­˜è·¯å¾„: {DATA_CACHE_PATH}")
print(f"{'='*80}")

print("\nğŸ”¥ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
print("1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir /root/autodl-tmp/training_logs")
print("2. åŠ è½½æ¨¡å‹æµ‹è¯•: FastLanguageModel.from_pretrained('/root/autodl-tmp/qwen25_grpo_model')")
print("3. æ£€æŸ¥è®­ç»ƒæ€»ç»“: cat /root/autodl-tmp/training_summary.json")

print("\nâœ¨ è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼") 