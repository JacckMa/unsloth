#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GSPO LoRAå¾®è°ƒ Qwen2.5-1.5B-Instruct æ¨¡å‹
æ•°æ®é›†: GSM8K
ä¿å­˜è·¯å¾„: /root/autodl-tmp
GSPO (Group Sequence Policy Optimization) - åºåˆ—çº§ä¼˜åŒ–ï¼Œä¸åŒäºGRPOçš„tokençº§ä¼˜åŒ–
"""

import os
import re
import json
import torch
import gc
from pathlib import Path
from datasets import load_dataset
from transformers import TrainingArguments
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ä¿å­˜æ ¹ç›®å½•
SAVE_ROOT = "/root/autodl-tmp"
os.makedirs(SAVE_ROOT, exist_ok=True)

# è®¾ç½®å„ç§ä¿å­˜è·¯å¾„
MODEL_SAVE_PATH = os.path.join(SAVE_ROOT, "qwen25_gspo_model")
CHECKPOINT_PATH = os.path.join(SAVE_ROOT, "qwen25_gspo_checkpoint") 
DATA_CACHE_PATH = os.path.join(SAVE_ROOT, "gsm8k_cache")
LOG_PATH = os.path.join(SAVE_ROOT, "training_logs")
MODEL_CACHE_PATH = os.path.join(SAVE_ROOT, "model_cache")  # é¢„è®­ç»ƒæ¨¡å‹ç¼“å­˜è·¯å¾„
LOCAL_MODEL_PATH = os.path.join(SAVE_ROOT, "models", "Qwen2.5-1.5B-Instruct")  # æœ¬åœ°æ¨¡å‹è·¯å¾„

# åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
for path in [MODEL_SAVE_PATH, CHECKPOINT_PATH, DATA_CACHE_PATH, LOG_PATH, MODEL_CACHE_PATH]:
    os.makedirs(path, exist_ok=True)

# æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
def check_local_model():
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(LOCAL_MODEL_PATH) and os.path.exists(os.path.join(LOCAL_MODEL_PATH, "config.json")):
        print(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH}")
        return True
    else:
        print(f"âŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python download_model.py")
        return False

# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
os.environ["HF_HOME"] = MODEL_CACHE_PATH
os.environ["TRANSFORMERS_CACHE"] = MODEL_CACHE_PATH
os.environ["HF_HUB_CACHE"] = MODEL_CACHE_PATH

print(f"ğŸš€ å¼€å§‹GSPOå¾®è°ƒ Qwen2.5-1.5B-Instruct")
print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")
print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„: {CHECKPOINT_PATH}")
print(f"ğŸ“Š æ—¥å¿—ä¿å­˜è·¯å¾„: {LOG_PATH}")
print(f"ğŸ—‚ï¸ æ•°æ®ç¼“å­˜è·¯å¾„: {DATA_CACHE_PATH}")
print(f"ğŸ¤— é¢„è®­ç»ƒæ¨¡å‹ç¼“å­˜è·¯å¾„: {MODEL_CACHE_PATH}")
print(f"ğŸ  æœ¬åœ°æ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
print(f"ğŸ”¥ GSPOç‰¹ç‚¹: åºåˆ—çº§ä¼˜åŒ–ï¼Œé¿å…tokençº§åˆ«å™ªå£°ï¼")

# æ£€æŸ¥æœ¬åœ°æ¨¡å‹
if not check_local_model():
    print("\nâš ï¸ è¯·å…ˆä¸‹è½½æ¨¡å‹:")
    print("   python download_model.py")
    print("\næˆ–è€…ä¿®æ”¹ä»£ç ä½¿ç”¨åœ¨çº¿ä¸‹è½½æ¨¡å¼")
    exit(1)

# ============================================================================
# 1. æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½
# ============================================================================

from unsloth import FastLanguageModel

max_seq_length = 2048
lora_rank = 32  # é€‚ä¸­çš„rankï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ

# å®šä¹‰æ ¼å¼åŒ–æ ‡ç­¾ï¼ˆç§»åˆ°å‰é¢é¿å…å˜é‡æœªå®šä¹‰é”™è¯¯ï¼‰
reasoning_start = "<start_working_out>"
reasoning_end = "<end_working_out>"
solution_start = "<SOLUTION>"
solution_end = "</SOLUTION>"

# æ·»åŠ è°ƒè¯•å˜é‡ï¼ˆå‚ç…§æµ‹è¯•æ–‡ä»¶ï¼‰
global PRINTED_TIMES
PRINTED_TIMES = 0
global PRINT_EVERY_STEPS
PRINT_EVERY_STEPS = 5

print(f"\n{'='*60}")
print("ğŸ”§ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
print(f"{'='*60}")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=LOCAL_MODEL_PATH,  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
    max_seq_length=max_seq_length,
    load_in_4bit=False,  # GSPOè®­ç»ƒæ—¶ä¸ä½¿ç”¨4bitï¼Œä¿æŒç²¾åº¦
    fast_inference=True,
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.7,
    # ä¸éœ€è¦cache_dirï¼Œå› ä¸ºç›´æ¥ä»æœ¬åœ°åŠ è½½
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# è®¾ç½®chat templateï¼ˆå‚ç…§æµ‹è¯•æ–‡ä»¶ï¼‰
chat_template = (
    "{% if messages[0]['role'] == 'system' %}"
    "{{ messages[0]['content'] + eos_token }}"
    "{% set loop_messages = messages[1:] %}"
    "{% else %}"
    "{{ '{system_prompt}' + eos_token }}"
    "{% set loop_messages = messages %}"
    "{% endif %}"
    "{% for message in loop_messages %}"
    "{% if message['role'] == 'user' %}"
    "{{ message['content'] }}"
    "{% elif message['role'] == 'assistant' %}"
    "{{ message['content'] + eos_token }}"
    "{% endif %}"
    "{% endfor %}"
    "{% if add_generation_prompt %}{{ '{reasoning_start}' }}"
    "{% endif %}"
)

system_prompt = f"""You are given a problem.
Think about the problem and provide your working out.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between {solution_start}{solution_end}"""

chat_template = chat_template.replace(
    "'{system_prompt}'", f"'{system_prompt}'"
).replace("'{reasoning_start}'", f"'{reasoning_start}'")
tokenizer.chat_template = chat_template

# ç¡®ä¿tokenizeré…ç½®æ­£ç¡®ï¼Œè§£å†³BOS tokenè­¦å‘Š
if tokenizer.bos_token is None:
    tokenizer.bos_token = tokenizer.eos_token
if tokenizer.bos_token_id is None:
    tokenizer.bos_token_id = tokenizer.eos_token_id

print("âœ… Chat templateé…ç½®å®Œæˆï¼")

# ============================================================================
# 2. LoRAé…ç½®
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ¯ é…ç½®LoRA...")
print(f"{'='*60}")

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    lora_alpha=lora_rank * 2,
    use_gradient_checkpointing="unsloth",
    random_state=42,
)

print("âœ… LoRAé…ç½®å®Œæˆï¼")

# ============================================================================
# 3. æ•°æ®é›†å‡†å¤‡
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ“š å‡†å¤‡GSM8Kæ•°æ®é›†...")
print(f"{'='*60}")

def extract_answer_from_gsm8k(text):
    """ä»GSM8Kæ ¼å¼ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

def format_gsm8k_for_gspo(example):
    """å°†GSM8Kæ ¼å¼åŒ–ä¸ºGSPOè®­ç»ƒæ ¼å¼"""
    system_prompt = f"""You are given a problem.
Think about the problem and provide your working out.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between {solution_start}{solution_end}"""
    
    return {
        "prompt": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": example['question']},
        ],
        "answer": extract_answer_from_gsm8k(example["answer"]),
        "full_solution": example["answer"]  # ä¿ç•™å®Œæ•´è§£ç­”ç”¨äºå‚è€ƒ
    }

# åŠ è½½GSM8Kæ•°æ®é›†
print("ğŸ“¥ ä¸‹è½½GSM8Kæ•°æ®é›†...")
try:
    dataset = load_dataset("gsm8k", "main", split="train", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} æ¡è®­ç»ƒæ•°æ®")
except Exception as e:
    print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
    dataset = load_dataset("openai/gsm8k", "main", split="train", cache_dir=DATA_CACHE_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} æ¡è®­ç»ƒæ•°æ®")

# æ ¼å¼åŒ–æ•°æ®é›†
print("ğŸ”„ æ ¼å¼åŒ–æ•°æ®é›†...")
formatted_dataset = dataset.map(format_gsm8k_for_gspo, num_proc=4)

# åªä½¿ç”¨å‰1000æ¡æ•°æ®è¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
train_size = min(1000, len(formatted_dataset))
gsm8k_train = formatted_dataset.select(range(train_size))

print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼ä½¿ç”¨ {len(gsm8k_train)} æ¡æ•°æ®è¿›è¡Œè®­ç»ƒ")

# ============================================================================
# 4. å¥–åŠ±å‡½æ•°å®šä¹‰ï¼ˆGSPOä½¿ç”¨ç›¸åŒçš„å¥–åŠ±å‡½æ•°ï¼Œä½†åœ¨åºåˆ—çº§åˆ«åº”ç”¨ï¼‰
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ¯ å®šä¹‰GSPOå¥–åŠ±å‡½æ•°...")
print(f"{'='*60}")

def format_checker_exact(completions, **kwargs):
    """æ£€æŸ¥æ˜¯å¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼ˆGSPOåºåˆ—çº§æ£€æŸ¥ï¼‰"""
    scores = []
    for completion in completions:
        response = completion[0]["content"]
        
        # åˆ›å»ºæ ¼å¼åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
        match_format = re.compile(
            rf"{reasoning_end}.*?"
            rf"{solution_start}(.+?){solution_end}"
            rf"[\s]{{0,}}$",
            flags=re.MULTILINE | re.DOTALL,
        )
        
        score = 0
        if match_format.search(response) is not None:
            score += 3.0  # GSPO: åºåˆ—çº§å¥–åŠ±ï¼Œä¸è¿›è¡Œtokençº§åˆ†è§£
        scores.append(score)
    
    return scores

def format_checker_flexible(completions, **kwargs):
    """çµæ´»çš„æ ¼å¼æ£€æŸ¥ï¼ˆGSPOåºåˆ—çº§ï¼‰"""
    scores = []
    for completion in completions:
        response = completion[0]["content"]
        
        score = 0
        # GSPO: åœ¨åºåˆ—çº§åˆ«ç´¯ç§¯åˆ†æ•°ï¼Œè€Œä¸æ˜¯tokençº§
        score += 0.5 if response.count(reasoning_end) == 1 else -1.0
        score += 0.5 if response.count(solution_start) == 1 else -1.0
        score += 0.5 if response.count(solution_end) == 1 else -1.0
        
        scores.append(score)
    
    return scores

def answer_correctness_checker(prompts, completions, answer, **kwargs):
    """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆGSPOåºåˆ—çº§è¯„ä¼°ï¼‰"""
    question = prompts[0][-1]["content"]
    responses = [completion[0]["content"] for completion in completions]
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç­”æ¡ˆ
    match_format = re.compile(
        rf"{reasoning_end}.*?"
        rf"{solution_start}(.+?){solution_end}"
        rf"[\s]{{0,}}$",
        flags=re.MULTILINE | re.DOTALL,
    )
    
    extracted_responses = [
        guess.group(1) if (guess := match_format.search(r)) is not None else None
        for r in responses
    ]
    
    scores = []
    for guess, true_answer in zip(extracted_responses, answer):
        score = 0
        if guess is None:
            scores.append(-2.0)
            continue
        if guess == true_answer:
            score += 5.0  # GSPO: åºåˆ—çº§æ­£ç¡®æ€§å¥–åŠ±
        elif guess.strip() == true_answer.strip():
            score += 3.5
        else:
            try:
                ratio = float(guess) / float(true_answer)
                if ratio >= 0.9 and ratio <= 1.1:
                    score += 2.0
                elif ratio >= 0.8 and ratio <= 1.2:
                    score += 1.5
                else:
                    score -= 2.5
            except:
                score -= 4.5
        scores.append(score)
    return scores

def reasoning_quality_checker(prompts, completions, answer, **kwargs):
    """æ£€æŸ¥æ¨ç†è´¨é‡ - GSPOåºåˆ—çº§è¯„ä¼°"""
    question = prompts[0][-1]["content"]
    responses = [completion[0]["content"] for completion in completions]
    
    # åŒ¹é…æ•°å­—çš„æ­£åˆ™è¡¨è¾¾å¼
    match_numbers = re.compile(
        solution_start + r".*?[\s]{0,}([-]?[\d\.\,]{1,})", flags=re.MULTILINE | re.DOTALL
    )
    
    extracted_responses = [
        guess.group(1) if (guess := match_numbers.search(r)) is not None else None
        for r in responses
    ]
    
    scores = []
    global PRINTED_TIMES
    global PRINT_EVERY_STEPS
    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
        print(
            "*" * 20 + f"GSPO Question:\n{question}",
            f"\nAnswer:\n{answer[0]}",
            f"\nResponse:\n{responses[0]}",
            f"\nExtracted:\n{extracted_responses[0]}",
        )
    PRINTED_TIMES += 1
    
    for guess, true_answer in zip(extracted_responses, answer):
        if guess is None:
            scores.append(-2.5)
            continue
        try:
            true_answer = float(true_answer.strip())
            guess = float(guess.strip().replace(",", ""))
            # GSPO: åºåˆ—çº§è´¨é‡è¯„åˆ†
            scores.append(3.5 if guess == true_answer else -1.5)
        except:
            scores.append(0)
            continue
    return scores

print("âœ… GSPOå¥–åŠ±å‡½æ•°å®šä¹‰å®Œæˆï¼")

# ============================================================================
# 5. GSPOè®­ç»ƒé…ç½®ï¼ˆå…³é”®å·®å¼‚ï¼šä½¿ç”¨PPOé…ç½®ä½†è®¾ç½®ä¸ºåºåˆ—çº§ä¼˜åŒ–ï¼‰
# ============================================================================

print(f"\n{'='*60}")
print("âš™ï¸ é…ç½®GSPOè®­ç»ƒå‚æ•°...")
print(f"{'='*60}")

# è®¡ç®—æœ€å¤§æç¤ºé•¿åº¦
sample_prompts = [tokenizer.apply_chat_template(gsm8k_train[i]["prompt"], 
                                               add_generation_prompt=True, 
                                               tokenize=True) 
                 for i in range(min(100, len(gsm8k_train)))]
prompt_lengths = [len(prompt) for prompt in sample_prompts]
max_prompt_length = max(prompt_lengths) + 50  # æ·»åŠ ä¸€äº›ç¼“å†²
max_completion_length = max_seq_length - max_prompt_length

print(f"ğŸ“ æœ€å¤§æç¤ºé•¿åº¦: {max_prompt_length}")
print(f"ğŸ“ æœ€å¤§å®Œæˆé•¿åº¦: {max_completion_length}")

# æ·»åŠ vllmé‡‡æ ·å‚æ•°
from unsloth import vLLMSamplingParams

# GSPOé‡‡æ ·å‚æ•°ï¼šæ›´é‡è§†åºåˆ—å¤šæ ·æ€§
vllm_sampling_params = vLLMSamplingParams(
    min_p=0.1,
    top_p=1.0,
    top_k=-1,
    seed=42,
    stop=[tokenizer.eos_token],
    include_stop_str_in_output=True,
)

# GSPOä½¿ç”¨PPOè®­ç»ƒå™¨ï¼Œä½†é…ç½®ä¸ºåºåˆ—çº§ä¼˜åŒ–
try:
    from trl import PPOConfig, PPOTrainer
    TRL_AVAILABLE = True
    print("âœ… TRL å·²å®‰è£…ï¼Œä½¿ç”¨PPOè®­ç»ƒå™¨è¿›è¡ŒGSPOè®­ç»ƒ")
except ImportError:
    print("âŒ TRL æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install trl")
    TRL_AVAILABLE = False
    raise ImportError("éœ€è¦å®‰è£… TRL åº“æ‰èƒ½è¿›è¡Œ GSPO è®­ç»ƒ")

# GSPOå…³é”®é…ç½®ï¼šåºåˆ—çº§ä¼˜åŒ–å‚æ•°
training_args = PPOConfig(
    # vllmé‡‡æ ·å‚æ•°
    vllm_sampling_params=vllm_sampling_params,
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    learning_rate=5e-6,  # GSPOæ¨èè¾ƒä½å­¦ä¹ ç‡ï¼Œé¿å…åºåˆ—çº§éœ‡è¡
    weight_decay=0.01,
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    optim="adamw_8bit",  # 8bitä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
    
    # æ‰¹æ¬¡å’Œæ¢¯åº¦å‚æ•°
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,  # GSPOå¢åŠ æ¢¯åº¦ç´¯ç§¯ä»¥ç¨³å®šåºåˆ—çº§æ›´æ–°
    
    # GSPOç‰¹å®šå‚æ•°ï¼šåºåˆ—çº§ä¼˜åŒ–
    batch_size=4,  # åºåˆ—ç»„å¤§å°ï¼Œç”¨äºç›¸å¯¹æ¯”è¾ƒ
    mini_batch_size=2,  # å°æ‰¹æ¬¡å¤§å°
    ppo_epochs=4,  # GSPOå»ºè®®æ›´å¤šepochä»¥å……åˆ†åˆ©ç”¨åºåˆ—çº§ä¿¡å·
    
    # è£å‰ªå‚æ•°ï¼ˆGSPOçš„å…³é”®ï¼šåºåˆ—çº§è£å‰ªï¼‰
    cliprange=0.2,  # é‡è¦æ€§æ¯”ç‡è£å‰ªèŒƒå›´
    cliprange_value=0.2,  # å€¼å‡½æ•°è£å‰ªèŒƒå›´
    vf_coef=0.1,  # å€¼å‡½æ•°æŸå¤±ç³»æ•°ï¼ˆGSPOä¸­è¾ƒå°ï¼Œå› ä¸ºé‡ç‚¹åœ¨åºåˆ—çº§ç­–ç•¥ï¼‰
    
    # åºåˆ—é•¿åº¦å‚æ•°
    max_prompt_length=max_prompt_length,
    max_length=max_seq_length,
    
    # è®­ç»ƒæ­¥æ•°å’Œä¿å­˜
    total_ppo_epochs=1000,  # æ€»è®­ç»ƒepoch
    save_freq=200,  # ä¿å­˜é¢‘ç‡
    log_freq=1,
    
    # è¾“å‡ºå’Œæ—¥å¿—
    project_kwargs={"project_name": "gspo_qwen25_gsm8k"},
    tracker_project_name="gspo_training",
    
    # GSPOçš„KLæ•£åº¦æ§åˆ¶ï¼ˆåºåˆ—çº§ï¼‰
    init_kl_coef=0.2,  # åˆå§‹KLç³»æ•°
    target=6,  # ç›®æ ‡KLæ•£åº¦
    horizon=10000,  # KLæ§åˆ¶æ—¶é—´çª—å£
    
    # å…¶ä»–å‚æ•°
    seed=42,
    remove_unused_columns=False,
)

print("âœ… GSPOè®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼")
print("ğŸ”¥ å…³é”®ç‰¹ç‚¹ï¼šåºåˆ—çº§è£å‰ªå’Œä¼˜åŒ–ï¼Œé¿å…tokençº§å™ªå£°")

# ============================================================================
# 6. åˆ›å»ºGSPOè®­ç»ƒå™¨ï¼ˆä½¿ç”¨PPOä½†é…ç½®ä¸ºåºåˆ—çº§ï¼‰
# ============================================================================

print(f"\n{'='*60}")
print("ğŸš‚ åˆ›å»ºGSPOè®­ç»ƒå™¨ï¼ˆåŸºäºPPOçš„åºåˆ—çº§ä¼˜åŒ–ï¼‰...")
print(f"{'='*60}")

# å‡†å¤‡å¥–åŠ±å‡½æ•°ç»„åˆ
def combined_reward_function(prompts, completions, **kwargs):
    """ç»„åˆå¥–åŠ±å‡½æ•°ï¼Œè¿”å›åºåˆ—çº§å¥–åŠ±"""
    # è·å–æ‰€æœ‰å¥–åŠ±åˆ†æ•°
    format_exact_scores = format_checker_exact(completions, **kwargs)
    format_flexible_scores = format_checker_flexible(completions, **kwargs)
    correctness_scores = answer_correctness_checker(prompts, completions, **kwargs)
    quality_scores = reasoning_quality_checker(prompts, completions, **kwargs)
    
    # åºåˆ—çº§å¥–åŠ±ç»„åˆï¼ˆæƒé‡å¯è°ƒï¼‰
    combined_scores = []
    for i in range(len(completions)):
        # GSPO: åœ¨åºåˆ—çº§åˆ«ç»„åˆå¥–åŠ±ï¼Œä¸è¿›è¡Œtokençº§åˆ†è§£
        total_score = (
            format_exact_scores[i] * 1.0 +      # æ ¼å¼å¥–åŠ±æƒé‡
            format_flexible_scores[i] * 0.5 +   # çµæ´»æ ¼å¼æƒé‡  
            correctness_scores[i] * 2.0 +       # æ­£ç¡®æ€§å¥–åŠ±æƒé‡ï¼ˆæœ€é«˜ï¼‰
            quality_scores[i] * 1.5             # è´¨é‡å¥–åŠ±æƒé‡
        )
        combined_scores.append(total_score)
    
    return combined_scores

# è½¬æ¢æ•°æ®é›†æ ¼å¼ä¸ºPPOè®­ç»ƒå™¨æœŸæœ›çš„æ ¼å¼
def convert_to_ppo_format(dataset):
    """è½¬æ¢æ•°æ®é›†ä¸ºPPOæ ¼å¼"""
    ppo_dataset = []
    for item in dataset:
        # å°†promptè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        prompt_text = tokenizer.apply_chat_template(
            item["prompt"], 
            add_generation_prompt=True, 
            tokenize=False
        )
        ppo_dataset.append({
            "query": prompt_text,
            "answer": item["answer"],
            "full_solution": item["full_solution"]
        })
    return ppo_dataset

ppo_formatted_dataset = convert_to_ppo_format(gsm8k_train)

# åˆ›å»ºPPOè®­ç»ƒå™¨è¿›è¡ŒGSPOè®­ç»ƒ
trainer = PPOTrainer(
    model=model,
    config=training_args,
    dataset=ppo_formatted_dataset,
    tokenizer=tokenizer,
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°†é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯æ¥å®ç°GSPOçš„åºåˆ—çº§ä¼˜åŒ–
)

print("âœ… GSPOè®­ç»ƒå™¨åˆ›å»ºå®Œæˆï¼")

# ============================================================================
# 7. GSPOè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼ˆå…³é”®ï¼šåºåˆ—çº§ä¼˜åŒ–å®ç°ï¼‰
# ============================================================================

print(f"\n{'='*60}")
print(f"ğŸš€ å¼€å§‹GSPOè®­ç»ƒï¼")
print(f"ğŸ“Š è®­ç»ƒæ•°æ®é‡: {len(ppo_formatted_dataset)}")
print(f"ğŸ”„ æœ€å¤§è®­ç»ƒepoch: {training_args.total_ppo_epochs}")
print(f"ğŸ’¾ ä¿å­˜é—´éš”: {training_args.save_freq} epoch")
print(f"ğŸ¯ GSPOç‰¹ç‚¹: åºåˆ—çº§ç­–ç•¥ä¼˜åŒ–")
print(f"{'='*60}")

# GSPOè®­ç»ƒå¾ªç¯
generation_kwargs = {
    "max_new_tokens": max_completion_length,
    "do_sample": True,
    "top_p": 1.0,
    "temperature": 1.0,
    "pad_token_id": tokenizer.pad_token_id,
}

try:
    from tqdm import tqdm
    
    # GSPOè®­ç»ƒä¸»å¾ªç¯
    for epoch in tqdm(range(training_args.total_ppo_epochs), desc="GSPO Training"):
        # æ‰¹æ¬¡è®­ç»ƒ
        for batch_idx, batch in enumerate(tqdm(trainer.dataloader, desc=f"Epoch {epoch}")):
            try:
                query_tensors = batch["input_ids"]
                
                # 1. ç”Ÿæˆå¤šä¸ªå€™é€‰åºåˆ—ï¼ˆGSPOçš„å…³é”®ï¼‰
                response_tensors = trainer.generate(
                    query_tensors, 
                    return_prompt=False,
                    **generation_kwargs
                )
                
                # 2. è§£ç ç”Ÿæˆçš„å“åº”
                batch["response"] = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) 
                                   for r in response_tensors]
                
                # 3. è®¡ç®—åºåˆ—çº§å¥–åŠ±
                completions = [[{"content": resp}] for resp in batch["response"]]
                prompts = [[{"content": q}] for q in batch["query"]]
                
                rewards = combined_reward_function(
                    prompts=prompts,
                    completions=completions,
                    answer=batch.get("answer", [0] * len(completions))
                )
                
                # è½¬æ¢ä¸ºtensor
                rewards = [torch.tensor(r, dtype=torch.float32) for r in rewards]
                
                # 4. GSPOåºåˆ—çº§ç­–ç•¥æ›´æ–°
                stats = trainer.step(query_tensors, response_tensors, rewards)
                
                # 5. è®°å½•ç»Ÿè®¡ä¿¡æ¯
                if batch_idx % training_args.log_freq == 0:
                    trainer.log_stats(stats, batch, rewards)
                    
                    # GSPOç‰¹å®šæ—¥å¿—
                    print(f"Epoch {epoch}, Batch {batch_idx}:")
                    print(f"  å¹³å‡åºåˆ—å¥–åŠ±: {torch.stack(rewards).mean():.4f}")
                    print(f"  å¥–åŠ±æ ‡å‡†å·®: {torch.stack(rewards).std():.4f}")
                    print(f"  åºåˆ—é•¿åº¦: {[len(r.squeeze()) for r in response_tensors]}")
                
            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå‡ºé”™: {e}")
                continue
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if epoch % training_args.save_freq == 0:
            save_path = os.path.join(CHECKPOINT_PATH, f"gspo_epoch_{epoch}")
            trainer.save_model(save_path)
            print(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {save_path}")
    
    print("âœ… GSPOè®­ç»ƒå®Œæˆï¼")
    
except Exception as e:
    print(f"âŒ GSPOè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    print("ğŸ’¾ å°è¯•ä¿å­˜å½“å‰çŠ¶æ€...")

# ============================================================================
# 8. ä¿å­˜æœ€ç»ˆGSPOæ¨¡å‹
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ’¾ ä¿å­˜æœ€ç»ˆGSPOæ¨¡å‹...")
print(f"{'='*60}")

try:
    # ä¿å­˜LoRAé€‚é…å™¨
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)
    print(f"âœ… GSPO LoRAæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—æ€»ç»“
    log_summary = {
        "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
        "dataset": "GSM8K",
        "training_method": "GSPO + LoRA",
        "optimization_level": "sequence_level",  # GSPOç‰¹ç‚¹
        "lora_rank": lora_rank,
        "max_seq_length": max_seq_length,
        "training_samples": len(ppo_formatted_dataset),
        "max_epochs": training_args.total_ppo_epochs,
        "learning_rate": training_args.learning_rate,
        "key_differences_from_grpo": [
            "åºåˆ—çº§ä¼˜åŒ–è€Œétokençº§",
            "åºåˆ—çº§é‡è¦æ€§æ¯”ç‡è£å‰ª",
            "é¿å…tokençº§æ¢¯åº¦å™ªå£°",
            "æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹"
        ],
        "save_paths": {
            "model": MODEL_SAVE_PATH,
            "checkpoint": CHECKPOINT_PATH,
            "logs": LOG_PATH,
            "data_cache": DATA_CACHE_PATH
        }
    }
    
    with open(os.path.join(SAVE_ROOT, "gspo_training_summary.json"), "w", encoding="utf-8") as f:
        json.dump(log_summary, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… GSPOè®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ°: {os.path.join(SAVE_ROOT, 'gspo_training_summary.json')}")
    
except Exception as e:
    print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

# ============================================================================
# 9. æ¸…ç†å†…å­˜
# ============================================================================

print(f"\n{'='*60}")
print("ğŸ§¹ æ¸…ç†å†…å­˜...")
print(f"{'='*60}")

del trainer
del model
del tokenizer
torch.cuda.empty_cache()
gc.collect()

print("âœ… å†…å­˜æ¸…ç†å®Œæˆï¼")

# ============================================================================
# 10. GSPOè®­ç»ƒå®Œæˆæ€»ç»“
# ============================================================================

print(f"\n{'='*80}")
print("ğŸ‰ GSPOå¾®è°ƒå®Œæˆï¼")
print(f"{'='*80}")
print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶éƒ½ä¿å­˜åœ¨: {SAVE_ROOT}")
print(f"ğŸ·ï¸ æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")
print(f"ğŸ’¾ æ£€æŸ¥ç‚¹è·¯å¾„: {CHECKPOINT_PATH}")
print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—è·¯å¾„: {LOG_PATH}")
print(f"ğŸ—‚ï¸ æ•°æ®ç¼“å­˜è·¯å¾„: {DATA_CACHE_PATH}")
print(f"{'='*80}")

print("\nğŸ”¥ GSPO vs GRPO å…³é”®åŒºåˆ«:")
print("1. ğŸ¯ ä¼˜åŒ–å±‚çº§: åºåˆ—çº§ vs tokençº§")
print("2. ğŸ“Š è£å‰ªç­–ç•¥: åºåˆ—çº§é‡è¦æ€§æ¯”ç‡è£å‰ª")
print("3. ğŸš€ è®­ç»ƒç¨³å®šæ€§: é¿å…tokençº§æ¢¯åº¦å™ªå£°")
print("4. ğŸ’¡ å¥–åŠ±åˆ†é…: åºåˆ—çº§ç»Ÿä¸€å¤„ç†")
print("5. ğŸ”§ é€‚ç”¨åœºæ™¯: é•¿åºåˆ—å’ŒMoEæ¨¡å‹æ›´ç¨³å®š")

print("\nğŸ”¥ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
print("1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir /root/autodl-tmp/training_logs")
print("2. åŠ è½½æ¨¡å‹æµ‹è¯•: FastLanguageModel.from_pretrained('/root/autodl-tmp/qwen25_gspo_model')")
print("3. æ£€æŸ¥è®­ç»ƒæ€»ç»“: cat /root/autodl-tmp/gspo_training_summary.json")
print("4. å¯¹æ¯”GRPOç»“æœ: æŸ¥çœ‹åºåˆ—çº§ä¼˜åŒ–çš„æ•ˆæœå·®å¼‚")

print("\nâœ¨ GSPOè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼") 
print("ğŸŠ äº«å—åºåˆ—çº§ä¼˜åŒ–å¸¦æ¥çš„è®­ç»ƒç¨³å®šæ€§æå‡ï¼") 